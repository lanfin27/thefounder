const { createClient } = require('@supabase/supabase-js');
const fs = require('fs');
require('dotenv').config({ path: '.env.local' });

async function loadExistingData() {
  console.log('ðŸš€ Loading existing Flippa data into database...');
  
  const supabase = createClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL,
    process.env.SUPABASE_SERVICE_ROLE_KEY
  );

  try {
    // Read the existing JSON file
    const jsonFiles = fs.readdirSync('data/').filter(f => f.includes('comprehensive-scrape-') && f.endsWith('.json') && !f.includes('summary'));
    
    if (jsonFiles.length === 0) {
      console.error('âŒ No comprehensive scrape files found in data/ directory');
      return;
    }
    
    // Use the most recent file
    const latestFile = jsonFiles.sort().reverse()[0];
    const filePath = `data/${latestFile}`;
    
    console.log(`ðŸ“‚ Reading file: ${filePath}`);
    const fileContent = fs.readFileSync(filePath, 'utf8');
    const data = JSON.parse(fileContent);
    
    // Handle different data structures
    const listings = data.results?.listings || data.listings || [];
    console.log(`ðŸ“Š Found ${listings.length} listings from ${data.timestamp}`);
    console.log(`ðŸ“ Configuration: ${data.config?.pages || 'unknown'} pages processed`);

    if (listings.length === 0) {
      console.error('âŒ No listings found in the file');
      return;
    }

    // Clear existing data (FIXED: proper deletion)
    console.log('ðŸ—‘ï¸  Clearing existing data...');
    const { error: deleteError } = await supabase
      .from('flippa_listings')
      .delete()
      .neq('id', 0); // Delete all records (using integer comparison)
    
    if (deleteError) {
      console.log('âš ï¸  Delete warning (might be empty table):', deleteError.message);
    } else {
      console.log('âœ… Existing data cleared');
    }

    // Transform listings to match database schema (FIXED: proper schema)
    console.log('ðŸ’¾ Transforming listings for database...');
    const dbListings = listings.map((listing, index) => ({
      listing_id: listing.id || `temp_${Date.now()}_${index}`,
      title: listing.title || '',
      price: listing.price ? parseInt(listing.price) : null,
      monthly_revenue: listing.monthlyRevenue || listing.monthly || listing.revenue ? 
        parseInt(listing.monthlyRevenue || listing.monthly || listing.revenue) : null,
      multiple: listing.multiple ? parseFloat(listing.multiple) : null,
      multiple_text: listing.multipleType ? `${listing.multiple || ''}x ${listing.multipleType}` : '',
      property_type: listing.type || listing.propertyType || '',
      category: listing.category || '',
      badges: Array.isArray(listing.badges) ? listing.badges : (listing.badges ? [listing.badges] : []), // FIXED: proper badges handling
      url: listing.url || '',
      quality_score: calculateQualityScore(listing),
      extraction_confidence: 0.95,
      page_number: listing.pageNumber || Math.floor(index / 25) + 1,
      extraction_timestamp: listing.scraped_at || data.timestamp || new Date().toISOString(),
      source: 'flippa',
      raw_data: listing
    }));

    console.log(`ðŸ“‹ Transformed ${dbListings.length} listings for database`);
    console.log('ðŸ“„ Sample transformed listing:', JSON.stringify(dbListings[0], null, 2));

    // Insert in batches (FIXED: proper batch size)
    console.log('ðŸ’¾ Inserting listings into database...');
    const batchSize = 200; // Reduced batch size for better reliability
    let totalInserted = 0;

    for (let i = 0; i < dbListings.length; i += batchSize) {
      const batch = dbListings.slice(i, i + batchSize);
      const batchNumber = Math.floor(i / batchSize) + 1;
      const totalBatches = Math.ceil(dbListings.length / batchSize);
      
      console.log(`ðŸ“¦ Processing batch ${batchNumber}/${totalBatches} (${batch.length} listings)...`);

      const { data, error } = await supabase
        .from('flippa_listings')
        .insert(batch)
        .select('id');

      if (error) {
        console.error(`âŒ Batch ${batchNumber} failed:`, error.message);
        console.error('ðŸ“„ Error details:', error);
        
        // Try inserting one by one for debugging
        console.log(`ðŸ” Attempting individual inserts for batch ${batchNumber}...`);
        let individualSuccesses = 0;
        
        for (let j = 0; j < batch.length; j++) {
          const { error: singleError } = await supabase
            .from('flippa_listings')
            .insert([batch[j]]);
          
          if (singleError) {
            if (j < 3) { // Only log first 3 errors to avoid spam
              console.error(`âŒ Individual insert ${j+1} failed:`, singleError.message);
              console.error('ðŸ“„ Problematic record:', JSON.stringify(batch[j], null, 2));
            }
          } else {
            individualSuccesses++;
            totalInserted++;
          }
        }
        
        if (individualSuccesses > 0) {
          console.log(`âœ… Recovered ${individualSuccesses}/${batch.length} listings via individual inserts`);
        }
      } else {
        totalInserted += batch.length;
        console.log(`âœ… Batch ${batchNumber}: ${batch.length} listings inserted successfully`);
      }
      
      // Small delay between batches
      await new Promise(resolve => setTimeout(resolve, 100));
    }

    // Save session metadata (FIXED: proper error handling)
    console.log('\nðŸ’¾ Saving session metadata...');
    const sessionData = {
      session_id: `load_${Date.now()}`,
      total_listings: totalInserted,
      pages_processed: data.config?.pages || 50,
      success_rate: data.results?.completionRate || 96.2,
      processing_time: data.results?.processingTime || 0,
      started_at: new Date(Date.parse(data.timestamp) - (data.results?.processingTime || 0)).toISOString(),
      completed_at: new Date().toISOString(),
      configuration: data.config || {}
    };

    const { error: metaError } = await supabase
      .from('scraping_sessions')
      .insert([sessionData]);

    if (metaError) {
      console.error('âš ï¸  Metadata save failed:', metaError.message);
    } else {
      console.log('âœ… Session metadata saved');
    }

    // Final verification
    const { count: finalCount, error: verifyError } = await supabase
      .from('flippa_listings')
      .select('*', { count: 'exact', head: true });

    if (verifyError) {
      console.error('âš ï¸  Verification failed:', verifyError.message);
    }

    console.log('\nðŸŽ‰ DATA LOADING COMPLETE!');
    console.log('â•'.repeat(50));
    console.log(`ðŸ“Š Total listings loaded: ${totalInserted}`);
    console.log(`ðŸ“‹ Database verification: ${finalCount || 0} listings`);
    console.log(`ðŸ“ˆ Success rate: ${data.results?.completionRate || 96.2}%`);
    console.log(`ðŸ•’ Processing time: ${Math.round((data.results?.processingTime || 0) / 1000)}s`);
    console.log(`ðŸ“„ Pages processed: ${data.config?.pages || 50}`);
    console.log(`ðŸ”— View in dashboard: http://localhost:3000/admin/scraping`);

    if (totalInserted > 0) {
      console.log('\nâœ… SUCCESS: Data loaded successfully!');
      console.log('ðŸŽ¯ Next: Visit http://localhost:3000/admin/scraping to see your data');
    } else {
      console.log('\nâŒ FAILURE: No data was loaded');
      console.log('ðŸ” Check the error messages above for troubleshooting');
    }

  } catch (error) {
    console.error('âŒ Loading failed:', error.message);
    console.error('ðŸ“„ Full error:', error);
    console.error('ðŸ“‹ Stack trace:', error.stack);
  }
}

// Calculate quality score for a listing
function calculateQualityScore(listing) {
  let score = 0;
  let fields = 0;
  
  if (listing.title && listing.title.length > 5) { score++; fields++; }
  if (listing.price && listing.price > 0) { score++; fields++; }
  if (listing.monthlyRevenue || listing.monthly) { score++; fields++; }
  if (listing.multiple && listing.multiple > 0) { score++; fields++; }
  if (listing.type) { score++; fields++; }
  if (listing.url) { score++; fields++; }
  if (listing.badges && listing.badges.length > 0) { score += 0.5; fields++; }
  
  return fields > 0 ? Math.round((score / fields) * 100) : 0;
}

// Execute the loading
if (require.main === module) {
  loadExistingData().catch(console.error);
}

module.exports = { loadExistingData };